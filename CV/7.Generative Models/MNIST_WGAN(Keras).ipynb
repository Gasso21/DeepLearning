{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43fd794e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# WGAN\n",
    "\n",
    "- **DCGAN의 문제점:** \n",
    "    1) discriminator와 generator 간의 균형을 유지하며 학습하기 어려움\n",
    "    2) 학습이 완료된 이후에도 mode dropping이 발생 (**mode collapsing**: 생성자가 하나의 최빈값에 치우쳐 변환)\n",
    "- **원인:** discriminator가 충분히 제 역할을 해주지 못해 모델이 최적점까지 학습되지 못함\n",
    "\n",
    "\n",
    "- **Wasserstein GAN의 차별점:**\n",
    "    1) discriminator 대신 새로 정의한 critic을 사용. discriminator는 Real/Fake를 판별하기 위해 'sigmoid'를 사용, output은 예측 확률값\n",
    "    2) critic은 EM(Earth Mover) distance로 부터 얻은 scalar값을 이용\n",
    "    3) EM distance는 확률분포 간의 거리를 측정하는 척도 (기존 척도인 KL divergence는 매우 strict하기 때문에 비연속적인 경우가 있어 학습에 어려움이 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6cc130",
   "metadata": {},
   "source": [
    "$\n",
    "\\textbf{Entropy}: H(q) = - \\sum\\limits^C_{c=1}q(y_c)log(q(y_c)) \\\\\n",
    "\\rightarrow C:\\text{범주의 개수} \\\\\n",
    "\\quad q:\\text{Probability mass function} \\\\\n",
    "\\textbf{Cross-entropy}: H_p(q) = - \\sum\\limits^C_{c=1}q(y_c)log(p(y_c)) \\\\\n",
    "\\rightarrow q: \\text{실제 분포} \\\\\n",
    "\\quad p: \\text{예측 분포} \\\\\n",
    "\\quad \\text{Training Data에서는 q를 알기 때문에 dissimilarity를 계산하는데 사용할 수 있음} \\\\\n",
    "\\quad \\textbf{Cross-entropy} > \\textbf{Entropy} \\\\\n",
    "\\textbf{Cross-Entropy of Loss Function}: -\\cfrac{1}{n}\\sum\\limits^n_{i=1}\\sum\\limits^C_{c=1}L_{ic}log(P_{ic})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017bf4d5",
   "metadata": {},
   "source": [
    "$\n",
    "\\textbf{Kullback-Leibler Divergence}: D_{KL}(q||p) = -\\sum\\limits^C_{c=1}q(y_c)[log(p(y_c))-log(q(y_c))] = H_p(q) - H(q) \\\\\n",
    "\\rightarrow Hp(q) \\geq H(q) \\\\\n",
    "\\quad \\text{예측분포 p를 실제분포 q에 가깝게 하는 것이 목표} \\\\\n",
    "\\quad \\text{cross-entropy를 최소화 시키는 것이 KL Divergence를 최소화시키는 것} \\\\\n",
    "\\quad \\text{이것이 불확실성을 제어하고자하는 예측모형의 실질적인 목적}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c05a22",
   "metadata": {},
   "source": [
    "$\n",
    "\\textbf{KL - Divergence }\\text{is not symmetric} \\\\\n",
    "\\rightarrow D_{KL}(P||Q) \\ne D_{KL}(Q||P) \\\\\n",
    "\\quad \\text{Can't use Distance Metric!}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f889f0",
   "metadata": {},
   "source": [
    "$\n",
    "\\textbf{Jensen - Shannon Divergence}: JSD(P,Q) = \\cfrac{1}{2}D_{KL}(P||M) + \\cfrac{1}{2}D_{KL}(Q||M) \\\\\n",
    "\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad where \\; M = \\cfrac{1}{2}(P+Q) \\\\\n",
    "\\rightarrow JSD(P,Q) = JSD(Q, P) \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c42b6",
   "metadata": {},
   "source": [
    "$\n",
    "\\textbf{The Total Variation(TV) distance}: \\delta(\\mathbb{P}_r,\\mathbb{P}_g) = \\sup\\limits_{A\\in\\sum}|\\mathbb{P}_r(A) - \\mathbb{P}_g(A)| \\\\\n",
    "\\rightarrow \\text{두 확률분포의 측정값이 벌어질 수 있는 가장 큰 값} \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa2edb6",
   "metadata": {},
   "source": [
    "$\n",
    "\\textbf{Earth-Mover(EM) distance or Wasserstein-1}: W(\\mathbb{P}_r,\\mathbb{P}_g) = \\inf\\limits_{\\gamma\\in\\prod(\\mathbb{P}_r,\\mathbb{P}_g)} \\mathbb{E}_{(x,y)\\thicksim\\gamma}[\\lVert x-y\\rVert] \\\\\n",
    "\\rightarrow \\text{두 확률분포의 결합확률분포 } \\prod(\\mathbb{P}_r,\\mathbb{P}_g) \\text{ 중에서 x와 y 거리의 기대값을 가장 작게 추정한 값} \\\\\n",
    "\\rightarrow \\text{얼마나 많은 질량 }\\gamma(x,y) \\text{를 } d=\\lVert x-y \\rVert \\text{만큼 옮겨야하는지에 대한 지표}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26997e65",
   "metadata": {},
   "source": [
    "### GAN에서의 거리함수\n",
    "$\n",
    "L^{(D)} = - \\int_x p_{data}(x)logD(x)dx - \\int_x p_g(x)log(1-D(x))dx \\\\\n",
    "\\rightarrow L^{(D)} = - \\int_x(p_{data}(x)logD(x) + p_g(x)log(1-D(x)))dx \\\\\n",
    "\\quad (y \\rightarrow alogy + blog(1-y)) \\\\\n",
    "\\rightarrow \\cfrac{a}{a+b} \\text{ 에서 최대값을 갖음} \\\\\n",
    "\\rightarrow D^{*}(x) = \\cfrac{p_{data}}{p_{data} + p_{g}} \\\\\n",
    "L^{(D^*)} = \\mathbb{E}_{x\\thicksim p_{data}}log\\cfrac{p_{data}}{p_{data}+p_g} - \\mathbb{E}_{x\\thicksim p_{g}}log\\cfrac{p_{g}}{p_{data}+p_g} \\\\\n",
    "\\rightarrow 2log2 - D_{KL}\\bigg[p_{data}\\Vert \\cfrac{p_{data}+p_g}{2}\\bigg] - D_{KL}\\bigg[p_{g}\\Vert \\cfrac{p_{data}+p_g}{2}\\bigg] \\\\\n",
    "\\rightarrow L^{(D^*)} = 2log2-2D_{JS}(p_{data} \\Vert p_g) \\\\\n",
    "\\rightarrow L^{(D^*)} \\text{를 최소화 한다는 것은 } D_{JS}(p_{data} \\Vert p_g) \\text{를 최대화 한다는 것} \\\\\n",
    "\\quad \\text{즉, 판별기가 실제 데이터에서 가짜 데이터를 정확하게 분류한다는 것을 의미함}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db9fac1",
   "metadata": {},
   "source": [
    "$\n",
    "\\text{최적의 생성기는 생성기 분포가 실제 데이터 분포와 동일한 경우일 때 형성} \\\\\n",
    "\\text{즉, } G^{(*)}(x) \\rightarrow p_g = p_{data} \\text{ 를 의미 }\n",
    "\\text{최적의 생성기가 주워졌을 때, 최적의 판별기는} \\\\\n",
    "\\rightarrow D^{(*)}(x) = \\cfrac{p_{data}}{p_{data} + p_{g}} = \\cfrac{1}{2} \\\\\n",
    "\\rightarrow L^{(*)} = 2log2 = 0.60\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f0383",
   "metadata": {},
   "source": [
    "### 두 분포가 겹치는 영역이 없는 경우\n",
    "$\n",
    "p_{data} = (x,y) \\quad where \\; x = 0, \\; y \\thicksim U(0,1) \\\\\n",
    "p_g = (x,y) \\quad where \\; x = \\theta, y \\thicksim U(0,1) \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abbfd6",
   "metadata": {},
   "source": [
    "$\n",
    "\\cdot D_{KL}(p_g \\Vert p_{data}) = \\mathbb{E}_{x=\\theta,y\\thicksim U(0,1)}\\log\\cfrac{p_g(x,y)}{p_{data}(x,y)} = \\sum 1 \\log\\cfrac{1}{0} = + \\infty \\\\\n",
    "\\cdot D_{JS}(p_g \\Vert p_{data}) = \\cfrac{1}{2}\\mathbb{E}_{x=0, y\\thicksim U(0,1)}\\log\\cfrac{p_{data}(x,y)}{\\cfrac{p_{data}(x,y) + p_g{(x,y)}}{2}} + \\cfrac{1}{2}\\mathbb{E}_{x=\\theta,y\\thicksim U(0,1)}\\log\\cfrac{p_g(x,y)}{\\cfrac{p_{data}(x,y)+p_g(x,y)}{2}} \\\\\n",
    "\\qquad\\qquad\\qquad = \\cfrac{1}{2}\\sum1\\log\\cfrac{1}{\\frac{1}{2}} + \\cfrac{1}{2}\\sum1\\log\\cfrac{1}{\\frac{1}{2}} = \\log2 \\\\\n",
    "\\cdot W(p_{data},p_g) = |\\theta|\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54423b97",
   "metadata": {
    "id": "54423b97"
   },
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67a96400",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67a96400",
    "outputId": "53dd8d06-51d8-4bbc-e001-e7ed961a55b7"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41fc9336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from keras.layers import concatenate, Dense, Reshape, BatchNormalization, Activation, Conv2DTranspose\n",
    "from keras.layers import Conv2D, LeakyReLU, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "591a5d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a5c8fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(generator,\n",
    "                noise_input,\n",
    "                noise_label=None,\n",
    "                noise_codes=None,\n",
    "                show=False,\n",
    "                step=0,\n",
    "                model_name=\"gan\"):\n",
    "    \"\"\"\n",
    "    # Arguments\n",
    "        generator (Model)\n",
    "        noise_input (ndarray)\n",
    "        show (bool)\n",
    "        step (int)\n",
    "        model_name (string)\n",
    "\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(model_name, \"generated\")\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "    \n",
    "    filename = os.path.join(filepath, \"%05d.png\" %step)\n",
    "    \n",
    "    rows = int(math.sqrt(noise_input.shape[0]))\n",
    "    if noise_label is not None:\n",
    "        noise_input = [noise_input, noise_label]\n",
    "        if noise_codes is not None:\n",
    "            noise_input += noise_codes\n",
    "\n",
    "    images = generator.predict(noise_input, verbose=None)\n",
    "    plt.figure(figsize=(2.2, 2.2))\n",
    "    num_images = images.shape[0]\n",
    "    image_size = images.shape[1]\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(rows, rows, i + 1)\n",
    "        image = np.reshape(images[i], [image_size, image_size])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e570dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(inputs, image_size):\n",
    "    # Stack: BN-ReLU-Conv2DTranspose\n",
    "    # inputs: z-vector(noise)\n",
    "    # image_size: Target size\n",
    "    # return: Model\n",
    "    \n",
    "    image_resize = image_size // 4\n",
    "    kernel_size = 5\n",
    "    layer_filters = [128, 64, 32, 1]\n",
    "    \n",
    "    x = Dense(image_resize*image_resize*layer_filters[0])(inputs)\n",
    "    x = Reshape((image_resize, image_resize, layer_filters[0]))(x)\n",
    "    \n",
    "    for filters in layer_filters:\n",
    "        # 1st, 2nd Conv layers: strides = 2\n",
    "        # 3rd, 4th Conv layers: strides = 1\n",
    "        if filters > layer_filters[-2]: #128, 64\n",
    "            strides = 2\n",
    "        else:\n",
    "            strides = 1\n",
    "        # BN-ReLU-Conv2DTranspose\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2DTranspose(filters=filters,\n",
    "                            kernel_size=kernel_size,\n",
    "                            strides=strides,\n",
    "                            padding='same'\n",
    "                           )(x)\n",
    "        \n",
    "    x = Activation('sigmoid')(x)\n",
    "    generator = keras.Model(inputs, x, name='generator')\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84275404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(inputs, activation='sigmoid'):\n",
    "    # BN으로는 수렴하지 않음\n",
    "    # Stack: LeakyReLU-Conv2D\n",
    "    # inputs: Image\n",
    "    # return: Model\n",
    "    \n",
    "    kernel_size = 5\n",
    "    layer_filters = [32, 64, 128, 256]\n",
    "    \n",
    "    x = inputs\n",
    "    for filters in layer_filters:\n",
    "        # 1st, 2nd, 3rd Conv layers: strides = 2\n",
    "        # 4th Conv layers: strides = 1\n",
    "        if filters == layer_filters[-1]:\n",
    "            strides = 1\n",
    "        else:\n",
    "            strides = 2\n",
    "        # LeakyReLU-Conv2D\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = Conv2D(filters=filters,\n",
    "                   kernel_size=kernel_size,\n",
    "                   strides=strides,\n",
    "                   padding='same'\n",
    "                  )(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1)(x)\n",
    "    x = Activation(activation)(x)\n",
    "    discriminator = keras.Model(inputs, x, name='discriminator')\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29dfbe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_label, y_pred):\n",
    "    return -K.mean(y_label * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9016a724",
   "metadata": {
    "id": "9016a724"
   },
   "outputs": [],
   "source": [
    "def build_and_train_models():\n",
    "    (x_train, _), (_, _) = keras.datasets.mnist.load_data()\n",
    "    \n",
    "    # Reshape & Normalize\n",
    "    image_size = x_train.shape[1]\n",
    "    x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
    "    x_train = x_train.astype('float32')/255\n",
    "    \n",
    "    model_name = 'MNIST_WGAN'\n",
    "    \n",
    "    # Additional wgan params\n",
    "    n_critic = 5\n",
    "    clip_value = 0.01\n",
    "    \n",
    "    # Network Params\n",
    "    latent_size = 100 # z-vector dimension\n",
    "    batch_size = 64\n",
    "    train_steps = 40000\n",
    "    lr = 5e-5 # DCGAN: 2e-4\n",
    "    # decay = 6e-8 # decays the lerning rate over time; DCGAN\n",
    "    input_shape = (image_size, image_size, 1)\n",
    "\n",
    "    # In Keras 2.11.0, 'decay' argument changed to 'weight_decay'\n",
    "    # Discriminator model\n",
    "    inputs = keras.Input(shape=input_shape, name='discriminator_input')\n",
    "    #discriminator = gan.discriminator(inputs, activation='linear')\n",
    "    discriminator = build_discriminator(inputs, activation='linear')\n",
    "    optimizer = keras.optimizers.legacy.RMSprop(learning_rate=lr)\n",
    "    discriminator.compile(loss=wasserstein_loss,\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=['accuracy']\n",
    "                         )\n",
    "    discriminator.summary()\n",
    "    \n",
    "    # Generator model\n",
    "    input_shape = (latent_size, )\n",
    "    inputs = keras.Input(shape=input_shape, name='z_input')\n",
    "    #generator = gan.generator(inputs, image_size)\n",
    "    generator = build_generator(inputs, image_size)\n",
    "    generator.summary()\n",
    "    \n",
    "    # Adversarial model\n",
    "    #optimizer = keras.optimizers.RMSprop(learning_rate=lr*0.5, decay=decay*0.5,)\n",
    "    discriminator.trainable = False\n",
    "    ## Adversarial = Generator + Discriminator\n",
    "    adversarial = keras.Model(inputs, discriminator(generator(inputs)), name='adversarial')\n",
    "    adversarial.compile(loss=wasserstein_loss,\n",
    "                        optimizer=optimizer,\n",
    "                        metrics=['accuracy']\n",
    "                       )\n",
    "    adversarial.summary()\n",
    "    \n",
    "    models = (generator, discriminator, adversarial)\n",
    "    params = (batch_size, latent_size, n_critic, clip_value, train_steps, model_name)\n",
    "    train(models, x_train, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d669a48",
   "metadata": {
    "id": "0d669a48"
   },
   "outputs": [],
   "source": [
    "def train(models, x_train, params):\n",
    "    # Discriminator와 Adversarial Model을 배치 단위로 번갈아 훈련\n",
    "    ## Discriminator는 제대로 레이블이 붙은 진짜와 가짜 이미지를 가지고 훈련\n",
    "    ## Adversarial는 진짜인 척하는 가짜 이미지로 훈련\n",
    "    \n",
    "    # GAN Model\n",
    "    generator, discriminator, adversarial = models\n",
    "    \n",
    "    # Network Params\n",
    "    # batch_size, latent_size, train_steps, model_name = params\n",
    "    batch_size, latent_size, n_critic, clip_value, train_steps, model_name = params\n",
    "    \n",
    "    # Save Generator Images every 500 epochs\n",
    "    save_interval = 500\n",
    "    \n",
    "    # Noise_Input\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size]) # 16 x 100\n",
    "    \n",
    "    train_size = x_train.shape[0]\n",
    "    real_labels = np.ones((batch_size, 1))\n",
    "    for i in range(train_steps): # train_steps: 40,000\n",
    "        \n",
    "        # First - Discriminator Train\n",
    "        # train discriminator n_critic times\n",
    "        loss = 0\n",
    "        acc = 0\n",
    "        for _ in range(n_critic):\n",
    "            # 1 batch of real (label=1.0) and fake images (label = -1.0)\n",
    "            # randomly pick real images from dataset\n",
    "            rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
    "            real_images = x_train[rand_indexes]\n",
    "            # generate fake images from noise using generator\n",
    "            # generate noise using uniform distribution\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "            fake_images = generator.predict(noise, verbose=None)\n",
    "        \n",
    "            # train the discriminator network\n",
    "            # real data label=1, fake data label=-1\n",
    "            # instead of 1 combined batch of real and fake images,\n",
    "            # train with 1 batch of real data first, then 1 batch of fake images.\n",
    "            # this tweak prevents the gradient from vanishing due to opposite signs of real and fake data labels\n",
    "            # and small magnitude of weights due to clipping.\n",
    "            real_loss, real_acc = discriminator.train_on_batch(real_images, real_labels)\n",
    "            fake_loss, fake_acc = discriminator.train_on_batch(fake_images, -real_labels)\n",
    "            \n",
    "            # accumulate average loss and accuracy\n",
    "            loss += 0.5 * (real_loss + fake_loss)\n",
    "            acc += 0.5 * (real_acc + fake_acc)\n",
    "            \n",
    "            # clip discriminator weights to satisfy Lipschitz constraint\n",
    "            for layer in discriminator.layers:\n",
    "                weights = layer.get_weights()\n",
    "                weights = [np.clip(weight, -clip_value, clip_value) for weight in weights]\n",
    "                layer.set_weights(weights)\n",
    "        \n",
    "        # average loss and accuracy per n_critic training iterations\n",
    "        loss /= n_critic\n",
    "        acc /= n_critic\n",
    "        log = \"%d: [discriminator loss: %f, acc: %f]\" %(i, loss, acc)\n",
    "        \n",
    "        # Second - Adversarial Train\n",
    "        # train the adversarial network for 1 batch\n",
    "        # 1 batch of fake images with label=1.0\n",
    "        # since the discriminator weights are frozen in adversarial network\n",
    "        # only the generator is trained\n",
    "        # generate noise using uniform distribution\n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "\n",
    "        # train the adversarial network\n",
    "        # note that unlike in discriminator training,\n",
    "        # we do not save the fake images in a variable\n",
    "        # the fake images go to the discriminator input of the adversarial\n",
    "        # for classification\n",
    "        # fake images are labelled as real\n",
    "        # log the loss and accuracy\n",
    "        loss, acc = adversarial.train_on_batch(noise, real_labels)\n",
    "        log = \"%s: [adversarial loss: %f, acc: %f]\" %(log, loss, acc)\n",
    "        if i % 100 == 0:\n",
    "            print(log)\n",
    "        \n",
    "        # Show generator images per 500 epochs\n",
    "        if (i+1) % save_interval == 0: # 500\n",
    "            if (i+1) == train_steps: # 40,000\n",
    "                show = True\n",
    "            else:\n",
    "                show = False\n",
    "                \n",
    "            # Make generator images per 500 epochs\n",
    "            plot_images(generator,\n",
    "                        noise_input=noise_input,\n",
    "                        show=show,\n",
    "                        step=(i+1),\n",
    "                        model_name=model_name\n",
    "                       )\n",
    "    \n",
    "    # save the model after training the generator\n",
    "    # the trained generator can be reloaded for future MNIST digit generation\n",
    "    generator.save(model_name + \"/mnist_wgan.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rdG9AgUWG1ez",
   "metadata": {
    "id": "rdG9AgUWG1ez",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " discriminator_input (InputL  [(None, 28, 28, 1)]      0         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 32)        832       \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 7, 7, 64)          51264     \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 4, 4, 128)         204928    \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 4097      \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,080,577\n",
      "Trainable params: 1,080,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_input (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6272)              633472    \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 7, 7, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 14, 14, 128)      409728    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 14, 14, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  (None, 28, 28, 64)       204864    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 28, 28, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_6 (Conv2DT  (None, 28, 28, 32)       51232     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 28, 28, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_7 (Conv2DT  (None, 28, 28, 1)        801       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,301,505\n",
      "Trainable params: 1,300,801\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n",
      "Model: \"adversarial\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_input (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " generator (Functional)      (None, 28, 28, 1)         1301505   \n",
      "                                                                 \n",
      " discriminator (Functional)  (None, 1)                 1080577   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,382,082\n",
      "Trainable params: 1,300,801\n",
      "Non-trainable params: 1,081,281\n",
      "_________________________________________________________________\n",
      "0: [discriminator loss: 0.015615, acc: 0.000000]: [adversarial loss: -0.000365, acc: 0.000000]\n",
      "100: [discriminator loss: -47.221006, acc: 0.042188]: [adversarial loss: 63.901520, acc: 0.000000]\n",
      "200: [discriminator loss: -6.859309, acc: 0.431250]: [adversarial loss: -20.959368, acc: 1.000000]\n",
      "300: [discriminator loss: 6.941396, acc: 0.500000]: [adversarial loss: -402.961548, acc: 1.000000]\n",
      "400: [discriminator loss: -2.675017, acc: 0.500000]: [adversarial loss: -24.478821, acc: 1.000000]\n",
      "500: [discriminator loss: -3.607877, acc: 0.500000]: [adversarial loss: -40.892365, acc: 1.000000]\n",
      "600: [discriminator loss: -3.144048, acc: 0.500000]: [adversarial loss: -44.277180, acc: 1.000000]\n",
      "700: [discriminator loss: -1.485925, acc: 0.500000]: [adversarial loss: -38.346508, acc: 1.000000]\n",
      "800: [discriminator loss: -0.337977, acc: 0.500000]: [adversarial loss: -30.313763, acc: 1.000000]\n",
      "900: [discriminator loss: -0.365533, acc: 0.500000]: [adversarial loss: -24.233391, acc: 1.000000]\n",
      "1000: [discriminator loss: -1.709254, acc: 0.500000]: [adversarial loss: -25.175501, acc: 1.000000]\n",
      "1100: [discriminator loss: -0.010090, acc: 0.500000]: [adversarial loss: -22.213259, acc: 1.000000]\n",
      "1200: [discriminator loss: 0.374054, acc: 0.500000]: [adversarial loss: -15.492761, acc: 1.000000]\n",
      "1300: [discriminator loss: -0.146695, acc: 0.500000]: [adversarial loss: -10.975752, acc: 1.000000]\n",
      "1400: [discriminator loss: 0.098195, acc: 0.500000]: [adversarial loss: -7.391298, acc: 1.000000]\n",
      "1500: [discriminator loss: 0.103502, acc: 0.500000]: [adversarial loss: -2.536366, acc: 1.000000]\n",
      "1600: [discriminator loss: 0.003215, acc: 0.389062]: [adversarial loss: -0.425055, acc: 0.171875]\n",
      "1700: [discriminator loss: -0.010706, acc: 0.000000]: [adversarial loss: 0.016809, acc: 0.000000]\n",
      "1800: [discriminator loss: -0.009758, acc: 0.000000]: [adversarial loss: -0.014111, acc: 0.000000]\n",
      "1900: [discriminator loss: -0.016483, acc: 0.000000]: [adversarial loss: 0.015082, acc: 0.000000]\n",
      "2000: [discriminator loss: -0.019022, acc: 0.000000]: [adversarial loss: 0.033541, acc: 0.000000]\n",
      "2100: [discriminator loss: -0.038721, acc: 0.000000]: [adversarial loss: 0.102161, acc: 0.000000]\n",
      "2200: [discriminator loss: -0.035146, acc: 0.000000]: [adversarial loss: 0.082291, acc: 0.000000]\n",
      "2300: [discriminator loss: -0.055571, acc: 0.000000]: [adversarial loss: 0.133931, acc: 0.000000]\n",
      "2400: [discriminator loss: -0.047554, acc: 0.000000]: [adversarial loss: 0.101735, acc: 0.000000]\n",
      "2500: [discriminator loss: -0.048238, acc: 0.003125]: [adversarial loss: 0.108955, acc: 0.000000]\n",
      "2600: [discriminator loss: -0.029484, acc: 0.009375]: [adversarial loss: 0.056181, acc: 0.000000]\n",
      "2700: [discriminator loss: -0.032463, acc: 0.000000]: [adversarial loss: 0.034420, acc: 0.000000]\n",
      "2800: [discriminator loss: -0.024328, acc: 0.004687]: [adversarial loss: 0.015999, acc: 0.000000]\n",
      "2900: [discriminator loss: -0.021948, acc: 0.000000]: [adversarial loss: -0.080893, acc: 0.000000]\n",
      "3000: [discriminator loss: -0.028598, acc: 0.000000]: [adversarial loss: 0.045922, acc: 0.000000]\n",
      "3100: [discriminator loss: -0.024647, acc: 0.000000]: [adversarial loss: 0.014863, acc: 0.000000]\n",
      "3200: [discriminator loss: -0.015151, acc: 0.000000]: [adversarial loss: 0.025604, acc: 0.000000]\n",
      "3300: [discriminator loss: -0.012681, acc: 0.000000]: [adversarial loss: 0.024788, acc: 0.000000]\n",
      "3400: [discriminator loss: -0.018267, acc: 0.000000]: [adversarial loss: 0.003109, acc: 0.000000]\n",
      "3500: [discriminator loss: -0.014495, acc: 0.000000]: [adversarial loss: 0.031766, acc: 0.000000]\n",
      "3600: [discriminator loss: -0.014791, acc: 0.000000]: [adversarial loss: 0.012388, acc: 0.000000]\n",
      "3700: [discriminator loss: -0.014120, acc: 0.000000]: [adversarial loss: -0.006664, acc: 0.000000]\n",
      "3800: [discriminator loss: -0.010926, acc: 0.000000]: [adversarial loss: -0.016757, acc: 0.000000]\n",
      "3900: [discriminator loss: -0.015010, acc: 0.000000]: [adversarial loss: -0.004694, acc: 0.000000]\n",
      "4000: [discriminator loss: -0.020424, acc: 0.000000]: [adversarial loss: -0.015870, acc: 0.000000]\n",
      "4100: [discriminator loss: -0.020385, acc: 0.000000]: [adversarial loss: 0.018667, acc: 0.000000]\n",
      "4200: [discriminator loss: -0.004569, acc: 0.000000]: [adversarial loss: -0.069626, acc: 0.000000]\n",
      "4300: [discriminator loss: -0.016158, acc: 0.000000]: [adversarial loss: -0.014894, acc: 0.000000]\n",
      "4400: [discriminator loss: -0.016803, acc: 0.000000]: [adversarial loss: -0.028350, acc: 0.000000]\n",
      "4500: [discriminator loss: -0.010960, acc: 0.000000]: [adversarial loss: -0.018768, acc: 0.000000]\n",
      "4600: [discriminator loss: -0.015505, acc: 0.000000]: [adversarial loss: -0.021899, acc: 0.000000]\n",
      "4700: [discriminator loss: -0.007854, acc: 0.000000]: [adversarial loss: -0.012635, acc: 0.000000]\n",
      "4800: [discriminator loss: -0.015482, acc: 0.000000]: [adversarial loss: -0.042447, acc: 0.000000]\n",
      "4900: [discriminator loss: -0.006538, acc: 0.000000]: [adversarial loss: -0.016690, acc: 0.000000]\n",
      "5000: [discriminator loss: -0.007757, acc: 0.000000]: [adversarial loss: -0.038022, acc: 0.000000]\n",
      "5100: [discriminator loss: -0.009558, acc: 0.000000]: [adversarial loss: -0.026504, acc: 0.000000]\n",
      "5200: [discriminator loss: -0.010969, acc: 0.000000]: [adversarial loss: -0.038923, acc: 0.000000]\n",
      "5300: [discriminator loss: -0.006985, acc: 0.000000]: [adversarial loss: -0.022289, acc: 0.000000]\n",
      "5400: [discriminator loss: -0.009220, acc: 0.000000]: [adversarial loss: -0.030127, acc: 0.000000]\n",
      "5500: [discriminator loss: -0.003432, acc: 0.000000]: [adversarial loss: -0.032585, acc: 0.000000]\n",
      "5600: [discriminator loss: -0.010094, acc: 0.000000]: [adversarial loss: -0.002543, acc: 0.000000]\n",
      "5700: [discriminator loss: -0.010313, acc: 0.000000]: [adversarial loss: -0.032138, acc: 0.000000]\n",
      "5800: [discriminator loss: -0.007412, acc: 0.000000]: [adversarial loss: -0.025492, acc: 0.000000]\n",
      "5900: [discriminator loss: -0.011627, acc: 0.000000]: [adversarial loss: -0.003492, acc: 0.000000]\n",
      "6000: [discriminator loss: -0.009992, acc: 0.000000]: [adversarial loss: -0.020578, acc: 0.000000]\n",
      "6100: [discriminator loss: -0.009402, acc: 0.000000]: [adversarial loss: -0.015412, acc: 0.000000]\n",
      "6200: [discriminator loss: -0.010364, acc: 0.000000]: [adversarial loss: -0.014939, acc: 0.000000]\n",
      "6300: [discriminator loss: -0.009294, acc: 0.000000]: [adversarial loss: -0.020532, acc: 0.000000]\n",
      "6400: [discriminator loss: -0.010935, acc: 0.000000]: [adversarial loss: -0.031241, acc: 0.000000]\n",
      "6500: [discriminator loss: -0.009713, acc: 0.000000]: [adversarial loss: -0.016269, acc: 0.000000]\n",
      "6600: [discriminator loss: -0.008754, acc: 0.000000]: [adversarial loss: -0.029175, acc: 0.000000]\n",
      "6700: [discriminator loss: -0.008032, acc: 0.000000]: [adversarial loss: -0.024267, acc: 0.000000]\n",
      "6800: [discriminator loss: -0.004279, acc: 0.000000]: [adversarial loss: -0.019566, acc: 0.000000]\n",
      "6900: [discriminator loss: -0.005287, acc: 0.000000]: [adversarial loss: -0.026938, acc: 0.000000]\n",
      "7000: [discriminator loss: -0.009976, acc: 0.000000]: [adversarial loss: -0.020743, acc: 0.000000]\n",
      "7100: [discriminator loss: -0.008234, acc: 0.000000]: [adversarial loss: -0.027276, acc: 0.000000]\n",
      "7200: [discriminator loss: -0.006192, acc: 0.000000]: [adversarial loss: -0.021362, acc: 0.000000]\n",
      "7300: [discriminator loss: -0.011184, acc: 0.000000]: [adversarial loss: -0.023080, acc: 0.000000]\n",
      "7400: [discriminator loss: -0.003403, acc: 0.000000]: [adversarial loss: -0.019421, acc: 0.000000]\n",
      "7500: [discriminator loss: -0.007697, acc: 0.000000]: [adversarial loss: -0.032142, acc: 0.000000]\n",
      "7600: [discriminator loss: -0.012947, acc: 0.000000]: [adversarial loss: -0.008857, acc: 0.000000]\n",
      "7700: [discriminator loss: -0.003222, acc: 0.000000]: [adversarial loss: -0.014595, acc: 0.000000]\n",
      "7800: [discriminator loss: -0.005233, acc: 0.000000]: [adversarial loss: -0.017369, acc: 0.000000]\n",
      "7900: [discriminator loss: -0.006291, acc: 0.000000]: [adversarial loss: -0.018586, acc: 0.000000]\n",
      "8000: [discriminator loss: -0.008000, acc: 0.000000]: [adversarial loss: -0.036045, acc: 0.000000]\n",
      "8100: [discriminator loss: -0.003432, acc: 0.000000]: [adversarial loss: -0.033893, acc: 0.000000]\n",
      "8200: [discriminator loss: -0.009324, acc: 0.000000]: [adversarial loss: -0.035387, acc: 0.000000]\n",
      "8300: [discriminator loss: -0.006167, acc: 0.000000]: [adversarial loss: -0.028294, acc: 0.000000]\n",
      "8400: [discriminator loss: -0.007601, acc: 0.000000]: [adversarial loss: -0.032092, acc: 0.000000]\n",
      "8500: [discriminator loss: -0.004696, acc: 0.000000]: [adversarial loss: -0.021156, acc: 0.000000]\n",
      "8600: [discriminator loss: -0.008756, acc: 0.000000]: [adversarial loss: -0.009212, acc: 0.000000]\n",
      "8700: [discriminator loss: -0.003078, acc: 0.000000]: [adversarial loss: -0.038402, acc: 0.000000]\n",
      "8800: [discriminator loss: -0.004265, acc: 0.000000]: [adversarial loss: -0.015572, acc: 0.000000]\n",
      "8900: [discriminator loss: -0.005780, acc: 0.000000]: [adversarial loss: -0.015513, acc: 0.000000]\n",
      "9000: [discriminator loss: -0.006226, acc: 0.000000]: [adversarial loss: -0.022396, acc: 0.000000]\n",
      "9100: [discriminator loss: -0.005082, acc: 0.000000]: [adversarial loss: -0.023585, acc: 0.000000]\n",
      "9200: [discriminator loss: -0.004763, acc: 0.000000]: [adversarial loss: -0.012663, acc: 0.000000]\n",
      "9300: [discriminator loss: -0.004736, acc: 0.000000]: [adversarial loss: -0.022003, acc: 0.000000]\n",
      "9400: [discriminator loss: -0.007590, acc: 0.000000]: [adversarial loss: -0.024507, acc: 0.000000]\n",
      "9500: [discriminator loss: -0.004942, acc: 0.000000]: [adversarial loss: -0.024172, acc: 0.000000]\n",
      "9600: [discriminator loss: -0.003212, acc: 0.000000]: [adversarial loss: -0.027206, acc: 0.000000]\n",
      "9700: [discriminator loss: -0.005642, acc: 0.000000]: [adversarial loss: -0.016096, acc: 0.000000]\n",
      "9800: [discriminator loss: -0.003715, acc: 0.000000]: [adversarial loss: -0.029098, acc: 0.000000]\n",
      "9900: [discriminator loss: -0.006072, acc: 0.000000]: [adversarial loss: -0.008419, acc: 0.000000]\n",
      "10000: [discriminator loss: -0.005814, acc: 0.000000]: [adversarial loss: -0.026202, acc: 0.000000]\n",
      "10100: [discriminator loss: -0.005395, acc: 0.000000]: [adversarial loss: -0.013563, acc: 0.000000]\n",
      "10200: [discriminator loss: -0.003026, acc: 0.000000]: [adversarial loss: -0.008606, acc: 0.000000]\n",
      "10300: [discriminator loss: -0.006561, acc: 0.000000]: [adversarial loss: -0.025113, acc: 0.000000]\n",
      "10400: [discriminator loss: -0.004322, acc: 0.000000]: [adversarial loss: -0.013862, acc: 0.000000]\n",
      "10500: [discriminator loss: -0.005593, acc: 0.000000]: [adversarial loss: -0.027312, acc: 0.000000]\n",
      "10600: [discriminator loss: -0.000831, acc: 0.000000]: [adversarial loss: -0.029842, acc: 0.000000]\n",
      "10700: [discriminator loss: -0.002145, acc: 0.000000]: [adversarial loss: -0.007935, acc: 0.000000]\n",
      "10800: [discriminator loss: -0.005441, acc: 0.000000]: [adversarial loss: -0.018534, acc: 0.000000]\n",
      "10900: [discriminator loss: -0.003117, acc: 0.000000]: [adversarial loss: -0.026654, acc: 0.000000]\n",
      "11000: [discriminator loss: -0.007263, acc: 0.000000]: [adversarial loss: -0.037356, acc: 0.000000]\n",
      "11100: [discriminator loss: -0.004139, acc: 0.000000]: [adversarial loss: -0.025185, acc: 0.000000]\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    build_and_train_models()\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    help_ = \"Load generator h5 model with trained weights\"\n",
    "    parser.add_argument(\"-g\", \"--generator\", help=help_)\n",
    "    args = parser.parse_args() # args에 위 내용 저장\n",
    "    if args.generator:\n",
    "        generator = load_model(args.generator)\n",
    "        gan.test_generator(generator)\n",
    "    else:\n",
    "        build_and_train_models()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d054543",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "1d054543",
    "outputId": "7b37cd10-13de-407b-c382-099ae58f3419"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "generator = load_model(\"./MNIST_WGAN/mnist_wgan.h5\")\n",
    "noise = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "gan.plot_images(generator,\n",
    "                noise_input=noise,\n",
    "                show=True,\n",
    "                model_name=\"./MNIST_WGAN/test_image\"\n",
    "               )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
